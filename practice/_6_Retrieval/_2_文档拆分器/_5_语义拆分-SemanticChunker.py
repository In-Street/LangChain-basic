"""
		SemanticChunker:
			æ ¹æ®æ–‡æœ¬çš„è¯­ä¹‰ç»“æ„è¿›è¡Œåˆ†å—ï¼Œä½¿æ¯ä¸ªåˆ†å—ä¿æŒè¯­ä¹‰çš„å®Œæ•´æ€§ï¼Œä»è€Œæé«˜æ£€ç´¢å¢å¼ºçš„æ•ˆæœ

			å‚æ•°ï¼š
				breakpoint_threshold_type: å®šä¹‰è¯­ä¹‰è¾¹ç•Œçš„æ£€æµ‹ç®—æ³•ï¼Œå†³å®šåˆ†å—çš„æ—¶æœºã€‚å–å€¼å¦‚ä¸‹ï¼š
						percentileï¼šè®¡ç®—ç›¸é‚»å¥å­åµŒå…¥å‘é‡çš„ä½™å¼¦è·ç¦»ï¼Œå–è·ç¦»åˆ†å¸ƒçš„ç¬¬Nç™¾åˆ†ä½å€¼ä½œä¸ºé˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼åˆ™åˆ†å‰²ã€‚
											é€‚ç”¨äº å¸¸è§„æ–‡æœ¬ï¼ˆæ–‡ç« ã€æŠ¥å‘Šï¼‰

						standard_deviation ï¼š ä»¥å‡å€¼ + Nå€æ ‡å‡†å·®ä¸ºé˜ˆå€¼ï¼Œè¯†åˆ«è¯­ä¹‰çªå˜ç‚¹
															é€‚ç”¨äº è¯­ä¹‰å˜åŒ–å‰§çƒˆçš„æ–‡æ¡£(å¦‚æŠ€æœ¯æ‰‹å†Œ)

						interquartileï¼š ç”¨å››åˆ†ä½è·(IQR) å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œï¼Œè¶…è¿‡åˆ™åˆ†å‰²
												é€‚ç”¨äº é•¿æ–‡æ¡£(å¦‚ä¹¦ç±)

						gradientï¼šåŸºäºåµŒå…¥å‘é‡å˜åŒ–çš„æ¢¯åº¦æ£€æµ‹åˆ†å‰²ç‚¹(éœ€è‡ªå®šä¹‰å®ç°ï¼‰
										é€‚ç”¨äº å®éªŒæ€§éœ€æ±‚

				breakpoint_threshold_amountï¼š æ–­ç‚¹é˜ˆå€¼ã€‚ æ§åˆ¶åˆ†å‰²çš„ç²’åº¦ï¼Œå€¼è¶Šå°åˆ†å‰²è¶Šç»†å—è¶Šå¤šï¼Œå€¼è¶Šå¤§åˆ†å‰²è¶Šç²—å—è¶Šå°‘
"""
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# åŠ è½½æ–‡æœ¬
with open("asset/load/09-ai1.txt", encoding="utf-8") as f:
	state_of_the_union = f.read()  # è¿”å›å­—ç¬¦ä¸²

# è·å–åµŒå…¥æ¨¡å‹
embed_model = OpenAIEmbeddings(
	model="text-embedding-3-large"
)
# è·å–åˆ‡å‰²å™¨
text_splitter = SemanticChunker(
	embeddings=embed_model,
	breakpoint_threshold_type="percentile",  # æ–­ç‚¹é˜ˆå€¼ç±»å‹ï¼šå­—é¢å€¼["ç™¾åˆ†ä½æ•°", "æ ‡å‡†å·®", "å››åˆ†ä½è·", "æ¢¯åº¦"] é€‰å…¶ä¸€
	breakpoint_threshold_amount=65.0  # æ–­ç‚¹é˜ˆå€¼æ•°é‡ (æä½é˜ˆå€¼ â†’ é«˜åˆ†å‰²æ•æ„Ÿåº¦ï¼Œå—è¶Šå¤š)ã€‚ è®¡ç®—ç›¸é‚»æ–‡æœ¬å‘é‡çš„ä½™å¼¦å¤¹è§’ï¼Œå½“å¤¹è§’å¤§äºæ­¤å€¼æ—¶ï¼Œä¼šåˆ‡åˆ°ä¸åŒçš„chunkä¸­
)
# åˆ‡åˆ†æ–‡æ¡£
docs = text_splitter.create_documents(texts=[state_of_the_union])
print(len(docs))
for doc in docs:
	print(f"ğŸ¦ æ–‡æ¡£ {doc}")
